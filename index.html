<!DOCTYPE html>
<html>
<head>
  <!-- Load Latex -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <meta charset="utf-8">
  <meta name="description"
        content="LIM: Large Interpolator Model for Dynamic Reconstruction">
  <meta name="keywords" content="LIM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LIM: Large Interpolator Model for Dynamic Reconstruction</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LIM: Large Interpolator Model for Dynamic Reconstruction</h1>
            <h2 class="title is-4 publication-title">Supplemental, Paper ID: 1064</h2>   
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        <p>
        Reconstructing dynamic assets from video data is central to many in computer vision and graphics tasks.
        Existing 4D reconstruction approaches are limited by category-specific models or slow optimization-based methods.
        Inspired by the recent Large Reconstruction Model (LRM), we present the <b>Large Interpolation Model</b> (<span class="lim">LIM</span>), a transformer-based feed-forward solution, guided by a novel causal consistency loss, for interpolating implicit 3D representations across time.
        Given implicit 3D representations at times \( t_{0} \) and \( t_{1} \), LIM produces a deformed shape at any continuous time \( t \in [t_{0},t_{1}] \)  delivering high-quality interpolations in seconds (per frame).
        Furthermore, LIM allows explicit mesh tracking across time, producing a consistently uv-textured mesh sequence ready for integration into existing production pipelines.
        We also use LIM, in conjunction with a diffusion-based multiview generator, to produce dynamic 4D reconstructions from monocular videos.
        We evaluate LIM on various dynamic datasets, benchmarking against image-space interpolation methods (e.g., FiLM) and direct triplane linear interpolation, and demonstrate clear advantages.
        In summary, LIM is the first feed-forward model capable of high-speed tracked 4D asset reconstruction across diverse categories.
    
    
    </p>
        </div>
          <img src="static/images/architecture.png" height="10%">
          <figcaption>
            Figure1: <b><span class="lim">LIM</span> Architecture</b>.
            (Left) Given multi-view images on 2 timesteps \( k \) and \( k+1 \), LIM interpolates any intermediate 3D representation at \( k+\alpha, \alpha \in [0,1] \). 
            It achieves this notably via cross-attention with the latest intermediate features of LRM on keyframe \( k \). 
            In practice, our LIM architecture has 6 blocks and LRM 12 blocks. 
            (Right) Block structure of LRM and LIM. 
            We include layer normalization before each module in blocks.
          </figcaption>
            <br>
          </div>
    </div>
  </div>
</section>
<!--/ Abstract. -->


<!-- Interpolation -->
<section class="section">  
  <div class="container is-max-desktop">
    <div class="container is-centered has-text-centered">
      <h2 class="title is-3">Frame Interpolation</h2>
      <div class="content is-centered has-text-centered has-text-justified">
        <p>
          Our first model, <span class="lim">LIM</span> directly interpolates color & opacity-field in 3D space in the multi-view setting.
          We observe that: 
          <b>(i)</b> Linear interpolation in triplane space fails on dynamic parts 
          <b>(ii)</b> Image-based interpolation (FILM) in the multi-view setting leads to defective reconstructions (with ghosting around dynamic parts)
          <b>(iii)</b> <span class="lim">LIM</span> yields the most plausible results, without artifacts.
        </p>
      </div>
      <!-- Add the video here -->
      <div class="video-container">
        <video autoplay loop muted class="is-centered">
          <source src="static/images/qualitative_interpolation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <figcaption>
        Figure2: <b>Frame Interpolation</b>. 
        We add 2 interpolated frames between each frame rendered from multi-view LRM.
        Oracle is the upper-bound limit, where we perform LRM reconstruction with multi-views images on all frames 
        (including the ones interpolated by the 3 methods).        
        For each method, we render the video with the LRM renders and the interpolated frames in-between. 
        We highlight a single interpolation step at the end.
      </figcaption>
      <!-- End video -->
    </div>
  </div>
</section>
  <!-- Interpolation -->


  <!-- Mesh Tracking -->
<section class="section">  
    <div class="container is-max-desktop">
      <div class="container is-centered has-text-centered">
        <h2 class="title is-3">Frame Tracking</h2>
        <div class="content is-centered has-text-centered has-text-justified">
          <p>
            An extension of our model, <span class="limxyz">LIM</span>, can trace the deformable shape through time.
            Given a multi-view sequence with RGB inputs, we start by extracting the triplane on the first frame with LRM.
            We render the triplane to obtain a depth map that we unproject to get multi-view renders of 
            <span class="blue">X</span><span class="magenta">Y</span><span class="yellow">Z</span>
            coordinates on the first (=canonical) frame. 
            Finally, from the XYZ renders on the first frame, and RGB inputs on all the frames, 
            <span class="limxyzblack">LIM</span> propagates the XYZ canonical coordinates on the first timestep to the other steps.
            We use these surface annotations to output a time-deforming mesh with fixed topology and texture.            
          <p>
        </div>
      <!-- Add the video here -->
      <div class="video-container">
        <video autoplay loop muted class="is-centered">
          <source src="static/images/qualitative_xyz.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <figcaption>
        Figure3: <b>XYZ coordinates tracking</b>.  
        <span class="limxyz">LIM</span> interpolates the XYZ coordinates on the first (=canonical) frame to the next ones.
      </figcaption>
      <!-- End video -->
      </div>
      </div>
    </div>
  </section>
  <!-- Mesh Tracking -->


<!-- 4D Reconstruction -->
<section class="section">  
  <div class="container is-max-desktop">
    <div class="container is-centered has-text-centered">
      <h2 class="title is-3">Application: Monocular Reconstruction</h2>
      <div class="content is-centered has-text-centered has-text-justified">
        <p>
          We show extension of our framework to the monocular reconstruction setting. 
          We observe that results from <b>TripoSR</b> (image-to-3D reconstructor) jitters since the frames are reconstructed independently.
          <b>Consistent4D</b> renders are consistent in time, but the method is slow.
          Our method combined with a monocular-to-multiview video diffusion model is consistent in time and significantly faster.
        </p>
      </div>
      <!-- Add the video here -->
      <div class="video-container">
        <video autoplay loop muted class="is-centered">
          <source src="static/images/qualitative_monocular.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <figcaption>Figure3: <b>Monocular Reconstruction</b> </figcaption>
      <!-- End video -->
    </div>
  </div>
</section>
<!-- 4D Reconstruction -->


  <!-- Related work -->
  <section class="section" id="Related work">
    <div class="container is-max-desktop content">
      <h2 class="title">Related Work</h2>
      <ul>
        <li>
          Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, Yao Yao (2023). 
          <b>Consistent4D</b>: Consistent 360deg Dynamic Object Generation from Monocular Video. 
          <a href="https://arxiv.org/abs/2311.02848"> (link)</a>
        </li>
        <li>
          Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao (2024). 
          <b>TripoSR</b>: Fast 3D Object Reconstruction from a Single Image. 
          <a href="https://arxiv.org/abs/2403.02151"> (link)</a>
        </li>
      </ul>
    </div>
  </section>
  <!-- Related work -->
  
  <footer>
    Template inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies.</a>
  </footer>


</body>
</html>
